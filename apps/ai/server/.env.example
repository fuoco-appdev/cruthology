#dmZocjY4b2hhYnEyMW1qbXBscGJ2cGE5anI6ZjA5M2U5NGUtZWE1Ny00ZWFjLTlkZGEtODQzMDljMjgzMWMy
# Path where models will be stored
# NOTE: This should be an absolute path and not relative path
MODEL_DIRECTORY="/home/ubuntu/model-cache"

# GPU id which nemo embedding ms will use
# EMBEDDING_MS_GPU_ID=0

# To control which GPU the vector database uses, specify the device ID.
# VECTORSTORE_GPU_DEVICE_ID=0

# GPU id which ranking ms will use (Make sure it is different from the one used for nim ms)
# RANKING_MS_GPU_ID=1

# Fill this out if you dont have a GPU. Leave this empty if you have a local GPU
NVIDIA_API_KEY=

# [OPTIONAL] the number of GPUs to make available to the inference server
# INFERENCE_GPU_COUNT="all"

# [OPTIONAL] the base directory inside which all persistent volumes will be created
# DOCKER_VOLUME_DIRECTORY="."


# name of the nemo embedding model
# Both arctic-embed-l & NV-Embed-QA are versions of e5-large-unsupervised
APP_EMBEDDINGS_MODELNAME="NV-Embed-QA"
EMBEDDING_MODEL_CKPT_NAME="snowflake-arctic-embed-l"
EMBEDDING_MODEL_PATH="https://huggingface.co/Snowflake/snowflake-arctic-embed-l"

# name of the nemo re-rank model
RANKING_MODEL_NAME="NV-Rerank-QA-Mistral-4B"
RANKING_MODEL_CKPT_NAME="nv-rerank-qa-mistral-4b_v2"
RANKING_MODEL_PATH="ohlfw0olaadg/ea-participants/nv-rerank-qa-mistral-4b:2"

# name of the nemo retriever pipeline one of ranked_hybrid or hybrid
NEMO_RETRIEVER_PIPELINE="ranked_hybrid"

# parameters for PGVector, update this when using PGVector Vector store
# POSTGRES_PASSWORD=password
# POSTGRES_USER=postgres
# POSTGRES_DB=api

# Update this line when using an external PGVector Vector store
# POSTGRES_HOST_IP=pgvector
# POSTGRES_PORT_NUMBER=5432

### Riva Parameters:

# Riva Speech API URI: Riva Server IP address/hostname and port
RIVA_API_URI=""

# [OPTIONAL] Riva Speech API Key
# If necessary, enter a key to access the Riva API
RIVA_API_KEY=""

# [OPTIONAL] Riva Function ID
# If necessary, enter a function ID to access the Riva API
RIVA_FUNCTION_ID=""

# TTS sample rate (Hz)
TTS_SAMPLE_RATE=48000

# the config file for the OpenTelemetry collector
OPENTELEMETRY_CONFIG_FILE="./configs/otel-collector-config.yaml"
# the config file for Jaeger
JAEGER_CONFIG_FILE="./configs/jaeger.yaml"

# [OPTIONAL] Set the logging level for the chain server. Possible values are NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL.
LOGLEVEL="INFO"
